{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19bc3e0d-2239-4ca0-996c-2a257af85e83",
   "metadata": {},
   "source": [
    "#1\n",
    "\n",
    "Min-Max scaling, also known as min-max normalization, is a data preprocessing technique used to scale numerical features in a dataset to a specific range, typically between 0 and 1. This scaling helps to ensure that all features have the same scale, which can be important for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940a17ad-b076-46bf-83df-f65496e9526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "#Here’s an example of how Min-Max scaling can be applied using Python’s:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "        \n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "scaler.fit(data)\n",
    "\n",
    "\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3866165c-8b15-43d7-97e1-6cb3b7bee859",
   "metadata": {},
   "source": [
    "#2\n",
    "\n",
    "Unit Vector Technique:\n",
    "\n",
    "In the Unit Vector technique, scaling is done considering the whole feature vector to be of unit length. This is achieved by dividing each observation vector by its norm (either the Manhattan distance (l1 norm) or the Euclidean distance (l2 norm) of the vector). This technique is particularly useful when dealing with features with hard boundaries.\n",
    "\n",
    "Difference between Unit Vector Technique and Min-Max Scaling:\n",
    "\n",
    "While both techniques scale values to a range of [0,1], they do so in different ways. The Unit Vector technique considers the whole feature vector for scaling, while Min-Max scaling operates on individual values within a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94602365-35c7-4372-8cfe-a8cf3c112499",
   "metadata": {},
   "source": [
    "#3\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It is often used to reduce the dimensionality of large datasets by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "PCA works by identifying a set of orthogonal axes, called principal components, that capture the maximum variance in the data. The first principal component captures the most variation in the data, but the second principal component captures the maximum variance that is orthogonal to the first principal component, and so on.\n",
    "\n",
    "\n",
    "For example, let’s consider a dataset with many features. If we feed our model with an excessively large dataset (with a large number of features/columns), it gives rise to overfitting, wherein the model starts getting influenced by outlier values and noise. This is where PCA comes in handy. It maps a higher dimensional feature space to a lower-dimensional feature space while ensuring that maximum information of the original dataset is retained in the dataset with reduced dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d07c6e-92a3-4ebd-975c-81614c4f2431",
   "metadata": {
    "tags": []
   },
   "source": [
    "#4\n",
    "\n",
    "\n",
    "### Relationship between PCA and Feature Extraction\n",
    "Feature Extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. PCA is a type of feature extraction technique that aims to reduce the number of input features while retaining as much of the original information as possible. It works on the condition that while the data in a higher-dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower-dimensional space should be maximum¹.\n",
    "\n",
    "### How PCA is used for Feature Extraction\n",
    "PCA uses an orthogonal transformation to convert a set of correlated variables to a set of uncorrelated variables. The main goal of PCA is to reduce the dimensionality of a dataset while preserving the most important patterns or relationships between the variables without any prior knowledge of the target variables. The new set of variables, smaller than the original set, retains most of the sample’s information, and is useful for regression and classification of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582a57e8-1dd1-4afa-93ad-c4bf230502ac",
   "metadata": {},
   "source": [
    "#5\n",
    "\n",
    "Min-Max scaling is a technique often used to normalize the range of independent variables or features of data. In this case, the features are price, rating, and delivery time. The goal of Min-Max scaling is to scale the features to a specific range, typically 0 to 1.\n",
    "\n",
    "Here's how you can apply Min-Max scaling to each feature:\n",
    "\n",
    "1. **Identify the minimum and maximum values of the feature**: For each feature, you need to find the minimum and maximum values in your dataset. For example, if you're looking at the 'price' feature, you would find the lowest and highest prices in your dataset.\n",
    "\n",
    "2. **Apply the Min-Max scaling formula**: The formula for Min-Max scaling is:\n",
    "\n",
    "    $$X_{new} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "    where `X` is an original value, `X_new` is the new value after scaling, `X_min` is the minimum value in the feature column, and `X_max` is the maximum value.\n",
    "\n",
    "3. **Replace original values with scaled values**: After calculating the new scaled value for each data point in your feature, replace the original values with these new ones. This will result in a distribution of values between 0 and 1.\n",
    "\n",
    "For example, let's say we have a dataset with 'price' ranging from $5 to $50. If we want to scale a price of $20 using Min-Max scaling, we would substitute these values into our formula:\n",
    "\n",
    "$$X_{new} = \\frac{20 - 5}{50 - 5} = 0.333$$\n",
    "\n",
    "So, the scaled price of $20 is approximately 0.333 when using Min-Max scaling.\n",
    "\n",
    "By applying this process to all features in your dataset (price, rating, delivery time), you ensure that all features have equal weight in your recommendation system model. This can help improve the performance of your model by preventing features with larger scales from dominating those with smaller scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c2d7429-6950-4a09-9594-46cb93cdde90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()\n",
    "Min_Max_scaling=min_max.fit_transform([[1, 5, 10, 15, 20]])\n",
    "print(Min_Max_scaling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
